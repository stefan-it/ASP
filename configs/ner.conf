hipe2022_ner {
  task = "ner"
  dataset = "hipe2022"

  data_dir = ${HOME}.flair/datasets/ner_hipe_2022/v2.1/ajmc/en/with_doc_seperator/
  model_dir = ${ASP}/output
  log_root = ${ASP}/output/logs

  max_segment_len = 256
  # Learning
  optimizer = "adamw"
  use_amp = true
  plm_learning_rate = 5e-5
  task_learning_rate = 1e-4
  plm_scheduler = "linear_with_warmup" # constant / constant_with_warmup / linear_with_warmup
  task_scheduler = "linear_with_warmup"
  warmup_ratio = 0.05
  
  adam_eps = 1e-8
  adam_weight_decay = 0.1

  max_grad_norm = 1  # Set 0 to disable clipping
  gradient_accumulation_steps = 1
  batch_size = 1
  num_epochs = 20

  # Model hyperparameters.
  activation = "relu"
  init_std = 0.02
  feature_emb_size = 20
  hidden_size = 150

  dropout_rate = 0.3

  # number of types
  # per, scope, work, loc, date, object and O
  num_typing_classes = 7
  # Other.
  beam_size = 1

  eval_frequency = 1000
  report_frequency = 50

  plm_tokenizer_name = bigscience-historical-texts/t5-efficient-blbooks-large-nl36
}

blbooks_t5_base = ${hipe2022_ner}{
  plm_learning_rate = 5e-5
  task_learning_rate = 3e-4
  
  hidden_size = 150
  
  plm_pretrained_name_or_path = bigscience-historical-texts/t5-efficient-blbooks-large-nl36
  
  eval_frequency = 1000
}